{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/hamid/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data Collection\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "\n",
    "#Save to a file\n",
    "with open('hamlet.txt', 'w') as file:\n",
    "    file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4818"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the dataset\n",
    "with open('hamlet.txt', 'r') as file:\n",
    "    text = file.read().lower()\n",
    "\n",
    "# Tokenize the text-- Creating indexes for words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index)+1\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 687],\n",
       " [1, 687, 4],\n",
       " [1, 687, 4, 45],\n",
       " [1, 687, 4, 45, 41],\n",
       " [1, 687, 4, 45, 41, 1886],\n",
       " [1, 687, 4, 45, 41, 1886, 1887],\n",
       " [1, 687, 4, 45, 41, 1886, 1887, 1888],\n",
       " [1180, 1889],\n",
       " [1180, 1889, 1890],\n",
       " [1180, 1889, 1890, 1891],\n",
       " [57, 407],\n",
       " [57, 407, 2],\n",
       " [57, 407, 2, 1181],\n",
       " [57, 407, 2, 1181, 177],\n",
       " [57, 407, 2, 1181, 177, 1892],\n",
       " [407, 1182],\n",
       " [407, 1182, 63],\n",
       " [408, 162],\n",
       " [408, 162, 377],\n",
       " [408, 162, 377, 21],\n",
       " [408, 162, 377, 21, 247],\n",
       " [408, 162, 377, 21, 247, 882],\n",
       " [18, 66],\n",
       " [451, 224],\n",
       " [451, 224, 248],\n",
       " [451, 224, 248, 1],\n",
       " [451, 224, 248, 1, 30],\n",
       " [408, 407],\n",
       " [451, 25],\n",
       " [408, 6],\n",
       " [408, 6, 43],\n",
       " [408, 6, 43, 62],\n",
       " [408, 6, 43, 62, 1893],\n",
       " [408, 6, 43, 62, 1893, 96],\n",
       " [408, 6, 43, 62, 1893, 96, 18],\n",
       " [408, 6, 43, 62, 1893, 96, 18, 566],\n",
       " [451, 71],\n",
       " [451, 71, 51],\n",
       " [451, 71, 51, 1894],\n",
       " [451, 71, 51, 1894, 567],\n",
       " [451, 71, 51, 1894, 567, 378],\n",
       " [451, 71, 51, 1894, 567, 378, 80],\n",
       " [451, 71, 51, 1894, 567, 378, 80, 3],\n",
       " [451, 71, 51, 1894, 567, 378, 80, 3, 273],\n",
       " [451, 71, 51, 1894, 567, 378, 80, 3, 273, 1181],\n",
       " [408, 20],\n",
       " [408, 20, 16],\n",
       " [408, 20, 16, 1895],\n",
       " [408, 20, 16, 1895, 114],\n",
       " [408, 20, 16, 1895, 114, 379],\n",
       " [408, 20, 16, 1895, 114, 379, 71],\n",
       " [408, 20, 16, 1895, 114, 379, 71, 883],\n",
       " [408, 20, 16, 1895, 114, 379, 71, 883, 491],\n",
       " [2, 5],\n",
       " [2, 5, 92],\n",
       " [2, 5, 92, 688],\n",
       " [2, 5, 92, 688, 58],\n",
       " [2, 5, 92, 688, 58, 144],\n",
       " [346, 29],\n",
       " [346, 29, 6],\n",
       " [346, 29, 6, 108],\n",
       " [346, 29, 6, 108, 568],\n",
       " [346, 29, 6, 108, 568, 884],\n",
       " [408, 14],\n",
       " [408, 14, 7],\n",
       " [408, 14, 7, 885],\n",
       " [408, 14, 7, 885, 1896],\n",
       " [346, 64],\n",
       " [346, 64, 380],\n",
       " [346, 64, 380, 37],\n",
       " [346, 64, 380, 37, 6],\n",
       " [346, 64, 380, 37, 6, 47],\n",
       " [346, 64, 380, 37, 6, 47, 689],\n",
       " [346, 64, 380, 37, 6, 47, 689, 120],\n",
       " [346, 64, 380, 37, 6, 47, 689, 120, 2],\n",
       " [347, 1],\n",
       " [347, 1, 1897],\n",
       " [347, 1, 1897, 4],\n",
       " [347, 1, 1897, 4, 8],\n",
       " [347, 1, 1897, 4, 8, 257],\n",
       " [347, 1, 1897, 4, 8, 257, 492],\n",
       " [347, 1, 1897, 4, 8, 257, 492, 68],\n",
       " [347, 1, 1897, 4, 8, 257, 492, 68, 87],\n",
       " [347, 1, 1897, 4, 8, 257, 492, 68, 87, 149],\n",
       " [57, 120],\n",
       " [57, 120, 2],\n",
       " [57, 120, 2, 347],\n",
       " [408, 5],\n",
       " [408, 5, 117],\n",
       " [408, 5, 117, 5],\n",
       " [408, 5, 117, 5, 139],\n",
       " [408, 5, 117, 5, 139, 68],\n",
       " [408, 5, 117, 5, 139, 68, 247],\n",
       " [408, 5, 117, 5, 139, 68, 247, 1182],\n",
       " [408, 5, 117, 5, 139, 68, 247, 1182, 63],\n",
       " [48, 205],\n",
       " [48, 205, 3],\n",
       " [48, 205, 3, 16],\n",
       " [48, 205, 3, 16, 409],\n",
       " [140, 2],\n",
       " [140, 2, 1898],\n",
       " [140, 2, 1898, 348],\n",
       " [140, 2, 1898, 348, 3],\n",
       " [140, 2, 1898, 348, 3, 1],\n",
       " [140, 2, 1898, 348, 3, 1, 493],\n",
       " [408, 79],\n",
       " [408, 79, 6],\n",
       " [408, 79, 6, 46],\n",
       " [408, 79, 6, 46, 124],\n",
       " [140, 125],\n",
       " [140, 125, 1899],\n",
       " [140, 125, 1899, 322],\n",
       " [140, 125, 1899, 322, 1183],\n",
       " [140, 125, 1899, 322, 1183, 121],\n",
       " [140, 125, 1899, 322, 1183, 121, 83],\n",
       " [140, 125, 1899, 322, 1183, 121, 83, 1900],\n",
       " [140, 125, 1899, 322, 1183, 121, 83, 1900, 6],\n",
       " [1901, 407],\n",
       " [1901, 407, 258],\n",
       " [1901, 407, 258, 8],\n",
       " [1901, 407, 258, 8, 494],\n",
       " [1901, 407, 258, 8, 494, 79],\n",
       " [1901, 407, 258, 8, 494, 79, 6],\n",
       " [1901, 407, 258, 8, 494, 79, 6, 380],\n",
       " [218, 408],\n",
       " [140, 1902],\n",
       " [140, 1902, 407],\n",
       " [451, 94],\n",
       " [451, 94, 24],\n",
       " [451, 94, 24, 13],\n",
       " [451, 94, 24, 13, 120],\n",
       " [451, 94, 24, 13, 120, 63],\n",
       " [48, 7],\n",
       " [48, 7, 495],\n",
       " [48, 7, 495, 4],\n",
       " [48, 7, 495, 4, 28],\n",
       " [451, 236],\n",
       " [451, 236, 120],\n",
       " [451, 236, 120, 236],\n",
       " [451, 236, 120, 236, 46],\n",
       " [451, 236, 120, 236, 46, 347],\n",
       " [140, 24],\n",
       " [140, 24, 258],\n",
       " [140, 24, 258, 16],\n",
       " [140, 24, 258, 16, 167],\n",
       " [140, 24, 258, 16, 167, 886],\n",
       " [140, 24, 258, 16, 167, 886, 131],\n",
       " [140, 24, 258, 16, 167, 886, 131, 3],\n",
       " [140, 24, 258, 16, 167, 886, 131, 3, 124],\n",
       " [451, 5],\n",
       " [451, 5, 29],\n",
       " [451, 5, 29, 189],\n",
       " [451, 5, 29, 189, 153],\n",
       " [140, 120],\n",
       " [140, 120, 887],\n",
       " [140, 120, 887, 71],\n",
       " [140, 120, 887, 71, 19],\n",
       " [140, 120, 887, 71, 19, 33],\n",
       " [140, 120, 887, 71, 19, 33, 1184],\n",
       " [2, 31],\n",
       " [2, 31, 14],\n",
       " [2, 31, 14, 50],\n",
       " [2, 31, 14, 50, 1903],\n",
       " [2, 31, 14, 50, 1903, 136],\n",
       " [2, 31, 14, 50, 1903, 136, 168],\n",
       " [2, 31, 14, 50, 1903, 136, 168, 4],\n",
       " [2, 31, 14, 50, 1903, 136, 168, 4, 28],\n",
       " [888, 16],\n",
       " [888, 16, 1904],\n",
       " [888, 16, 1904, 410],\n",
       " [888, 16, 1904, 410, 690],\n",
       " [888, 16, 1904, 410, 690, 189],\n",
       " [888, 16, 1904, 410, 690, 189, 4],\n",
       " [888, 16, 1904, 410, 690, 189, 4, 72],\n",
       " [259, 5],\n",
       " [259, 5, 29],\n",
       " [259, 5, 29, 1905],\n",
       " [259, 5, 29, 1905, 28],\n",
       " [259, 5, 29, 1905, 28, 889],\n",
       " [17, 72],\n",
       " [17, 72, 3],\n",
       " [17, 72, 3, 257],\n",
       " [17, 72, 3, 257, 1],\n",
       " [17, 72, 3, 257, 1, 1906],\n",
       " [17, 72, 3, 257, 1, 1906, 4],\n",
       " [17, 72, 3, 257, 1, 1906, 4, 16],\n",
       " [17, 72, 3, 257, 1, 1906, 4, 16, 124],\n",
       " [11, 37],\n",
       " [11, 37, 131],\n",
       " [11, 37, 131, 16],\n",
       " [11, 37, 131, 16, 1185],\n",
       " [11, 37, 131, 16, 1185, 43],\n",
       " [25, 70],\n",
       " [25, 70, 1907],\n",
       " [25, 70, 1907, 33],\n",
       " [25, 70, 1907, 33, 190],\n",
       " [25, 70, 1907, 33, 190, 2],\n",
       " [25, 70, 1907, 33, 190, 2, 85],\n",
       " [25, 70, 1907, 33, 190, 2, 85, 3],\n",
       " [25, 70, 1907, 33, 190, 2, 85, 3, 9],\n",
       " [48, 1186],\n",
       " [48, 1186, 1186],\n",
       " [48, 1186, 1186, 691],\n",
       " [48, 1186, 1186, 691, 14],\n",
       " [48, 1186, 1186, 691, 14, 890],\n",
       " [451, 411],\n",
       " [451, 411, 150],\n",
       " [451, 411, 150, 7],\n",
       " [451, 411, 150, 7, 237],\n",
       " [2, 50],\n",
       " [2, 50, 72],\n",
       " [2, 50, 72, 206],\n",
       " [2, 50, 72, 206, 131],\n",
       " [2, 50, 72, 206, 131, 1908],\n",
       " [2, 50, 72, 206, 131, 1908, 18],\n",
       " [2, 50, 72, 206, 131, 1908, 18, 381],\n",
       " [11, 36],\n",
       " [11, 36, 27],\n",
       " [11, 36, 27, 1909],\n",
       " [11, 36, 27, 1909, 191],\n",
       " [11, 36, 27, 1909, 191, 33],\n",
       " [11, 36, 27, 1909, 191, 33, 1187],\n",
       " [24, 34],\n",
       " [24, 34, 177],\n",
       " [24, 34, 177, 692],\n",
       " [24, 34, 177, 692, 29],\n",
       " [24, 34, 177, 692, 29, 189],\n",
       " [48, 64],\n",
       " [48, 64, 411],\n",
       " [48, 64, 411, 34],\n",
       " [48, 64, 411, 34, 150],\n",
       " [2, 50],\n",
       " [2, 50, 72],\n",
       " [2, 50, 72, 139],\n",
       " [2, 50, 72, 139, 407],\n",
       " [2, 50, 72, 139, 407, 85],\n",
       " [2, 50, 72, 139, 407, 85, 4],\n",
       " [2, 50, 72, 139, 407, 85, 4, 16],\n",
       " [346, 274],\n",
       " [346, 274, 124],\n",
       " [346, 274, 124, 4],\n",
       " [346, 274, 124, 4, 38],\n",
       " [95, 1910],\n",
       " [95, 1910, 275],\n",
       " [95, 1910, 275, 693],\n",
       " [95, 1910, 275, 693, 249],\n",
       " [95, 1910, 275, 693, 249, 1911],\n",
       " [95, 1910, 275, 693, 249, 1911, 56],\n",
       " [95, 1910, 275, 693, 249, 1911, 56, 1],\n",
       " [95, 1910, 275, 693, 249, 1911, 56, 1, 1188],\n",
       " [108, 147],\n",
       " [108, 147, 15],\n",
       " [108, 147, 15, 569],\n",
       " [108, 147, 15, 569, 694],\n",
       " [108, 147, 15, 569, 694, 1912],\n",
       " [108, 147, 15, 569, 694, 1912, 11],\n",
       " [108, 147, 15, 569, 694, 1912, 11, 219],\n",
       " [108, 147, 15, 569, 694, 1912, 11, 219, 4],\n",
       " [108, 147, 15, 569, 694, 1912, 11, 219, 4, 109],\n",
       " [89, 51],\n",
       " [89, 51, 9],\n",
       " [89, 51, 9, 1189],\n",
       " [89, 51, 9, 1189, 347],\n",
       " [89, 51, 9, 1189, 347, 2],\n",
       " [89, 51, 9, 1189, 347, 2, 8],\n",
       " [89, 51, 9, 1189, 347, 2, 8, 66],\n",
       " [1, 1190],\n",
       " [1, 1190, 39],\n",
       " [1, 1190, 39, 891],\n",
       " [1, 1190, 39, 891, 103],\n",
       " [140, 349],\n",
       " [140, 349, 382],\n",
       " [140, 349, 382, 80],\n",
       " [140, 349, 382, 80, 4],\n",
       " [57, 1],\n",
       " [57, 1, 183],\n",
       " [141, 89],\n",
       " [141, 89, 9],\n",
       " [141, 89, 9, 173],\n",
       " [141, 89, 9, 173, 131],\n",
       " [346, 10],\n",
       " [346, 10, 1],\n",
       " [346, 10, 1, 275],\n",
       " [346, 10, 1, 275, 695],\n",
       " [346, 10, 1, 275, 695, 61],\n",
       " [346, 10, 1, 275, 695, 61, 1],\n",
       " [346, 10, 1, 275, 695, 61, 1, 30],\n",
       " [346, 10, 1, 275, 695, 61, 1, 30, 249],\n",
       " [346, 10, 1, 275, 695, 61, 1, 30, 249, 145],\n",
       " [140, 42],\n",
       " [140, 42, 238],\n",
       " [140, 42, 238, 7],\n",
       " [140, 42, 238, 7, 1913],\n",
       " [140, 42, 238, 7, 1913, 85],\n",
       " [140, 42, 238, 7, 1913, 85, 3],\n",
       " [140, 42, 238, 7, 1913, 85, 3, 9],\n",
       " [140, 42, 238, 7, 1913, 85, 3, 9, 120],\n",
       " [346, 452],\n",
       " [346, 452, 9],\n",
       " [346, 452, 9, 14],\n",
       " [346, 452, 9, 14, 61],\n",
       " [346, 452, 9, 14, 61, 1],\n",
       " [346, 452, 9, 14, 61, 1, 30],\n",
       " [346, 452, 9, 14, 61, 1, 30, 276],\n",
       " [346, 452, 9, 14, 61, 1, 30, 276, 9],\n",
       " [346, 452, 9, 14, 61, 1, 30, 276, 9, 120],\n",
       " [383, 62],\n",
       " [383, 62, 61],\n",
       " [383, 62, 61, 9],\n",
       " [383, 62, 61, 9, 1914],\n",
       " [383, 62, 61, 9, 1914, 21],\n",
       " [383, 62, 61, 9, 1914, 21, 17],\n",
       " [383, 62, 61, 9, 1914, 21, 17, 1915],\n",
       " [383, 62, 61, 9, 1914, 21, 17, 1915, 696],\n",
       " [346, 9],\n",
       " [346, 9, 67],\n",
       " [346, 9, 67, 26],\n",
       " [346, 9, 67, 26, 1191],\n",
       " [346, 9, 67, 26, 1191, 90],\n",
       " [140, 260],\n",
       " [140, 260, 9],\n",
       " [140, 260, 9, 120],\n",
       " [48, 24],\n",
       " [48, 24, 238],\n",
       " [48, 24, 238, 42],\n",
       " [48, 24, 238, 42, 11],\n",
       " [48, 24, 238, 42, 11, 1916],\n",
       " [48, 24, 238, 42, 11, 1916, 16],\n",
       " [48, 24, 238, 42, 11, 1916, 16, 110],\n",
       " [48, 24, 238, 42, 11, 1916, 16, 110, 4],\n",
       " [48, 24, 238, 42, 11, 1916, 16, 110, 4, 124],\n",
       " [350, 17],\n",
       " [350, 17, 11],\n",
       " [350, 17, 11, 192],\n",
       " [350, 17, 11, 192, 2],\n",
       " [350, 17, 11, 192, 2, 697],\n",
       " [350, 17, 11, 192, 2, 697, 301],\n",
       " [10, 75],\n",
       " [10, 75, 1],\n",
       " [10, 75, 1, 412],\n",
       " [10, 75, 1, 412, 4],\n",
       " [10, 75, 1, 412, 4, 496],\n",
       " [10, 75, 1, 412, 4, 496, 193],\n",
       " [76, 570],\n",
       " [76, 570, 571],\n",
       " [76, 570, 571, 41],\n",
       " [76, 570, 571, 41, 109],\n",
       " [76, 570, 571, 41, 109, 5],\n",
       " [76, 570, 571, 41, 109, 5, 453],\n",
       " [76, 570, 571, 41, 109, 5, 453, 80],\n",
       " [76, 570, 571, 41, 109, 5, 453, 80, 85],\n",
       " [140, 9],\n",
       " [140, 9, 13],\n",
       " [140, 9, 13, 892],\n",
       " [346, 105],\n",
       " [346, 105, 9],\n",
       " [346, 105, 9, 1917],\n",
       " [346, 105, 9, 1917, 154],\n",
       " [48, 323],\n",
       " [48, 323, 85],\n",
       " [48, 323, 85, 85],\n",
       " [48, 323, 85, 85, 5],\n",
       " [48, 323, 85, 85, 5, 453],\n",
       " [48, 323, 85, 85, 5, 453, 80],\n",
       " [48, 323, 85, 85, 5, 453, 80, 85],\n",
       " [218, 1],\n",
       " [218, 1, 183],\n",
       " [140, 71],\n",
       " [140, 71, 277],\n",
       " [140, 71, 277, 2],\n",
       " [140, 71, 277, 2, 31],\n",
       " [140, 71, 277, 2, 31, 14],\n",
       " [140, 71, 277, 2, 31, 14, 377],\n",
       " [346, 53],\n",
       " [346, 53, 51],\n",
       " [346, 53, 51, 120],\n",
       " [346, 53, 51, 120, 6],\n",
       " [346, 53, 51, 120, 6, 1192],\n",
       " [346, 53, 51, 120, 6, 1192, 572],\n",
       " [346, 53, 51, 120, 6, 1192, 572, 413],\n",
       " [13, 14],\n",
       " [13, 14, 16],\n",
       " [13, 14, 16, 302],\n",
       " [13, 14, 16, 302, 54],\n",
       " [13, 14, 16, 302, 54, 39],\n",
       " [13, 14, 16, 302, 54, 39, 1184],\n",
       " [24, 117],\n",
       " [24, 117, 6],\n",
       " [24, 117, 6, 414],\n",
       " [48, 225],\n",
       " [48, 225, 8],\n",
       " [48, 225, 8, 163],\n",
       " [48, 225, 8, 163, 5],\n",
       " [48, 225, 8, 163, 5, 146],\n",
       " [48, 225, 8, 163, 5, 146, 14],\n",
       " [48, 225, 8, 163, 5, 146, 14, 16],\n",
       " [48, 225, 8, 163, 5, 146, 14, 16, 226],\n",
       " [278, 1],\n",
       " [278, 1, 1193],\n",
       " [278, 1, 1193, 2],\n",
       " [278, 1, 1193, 2, 178],\n",
       " [278, 1, 1193, 2, 178, 1918],\n",
       " [4, 111],\n",
       " [4, 111, 142],\n",
       " [4, 111, 142, 190],\n",
       " [140, 13],\n",
       " [140, 13, 9],\n",
       " [140, 13, 9, 14],\n",
       " [140, 13, 9, 14, 61],\n",
       " [140, 13, 9, 14, 61, 1],\n",
       " [140, 13, 9, 14, 61, 1, 30],\n",
       " [48, 23],\n",
       " [48, 23, 42],\n",
       " [48, 23, 42, 238],\n",
       " [48, 23, 42, 238, 3],\n",
       " [48, 23, 42, 238, 3, 52],\n",
       " [48, 23, 42, 238, 3, 52, 66],\n",
       " [93, 59],\n",
       " [93, 59, 1],\n",
       " [93, 59, 1, 99],\n",
       " [93, 59, 1, 99, 1194],\n",
       " [93, 59, 1, 99, 1194, 25],\n",
       " [93, 59, 1, 99, 1194, 25, 108],\n",
       " [93, 59, 1, 99, 1194, 25, 108, 35],\n",
       " [95, 134],\n",
       " [95, 134, 893],\n",
       " [95, 134, 893, 573],\n",
       " [95, 134, 893, 573, 1919],\n",
       " [27, 1920],\n",
       " [27, 1920, 25],\n",
       " [27, 1920, 25, 206],\n",
       " [27, 1920, 25, 206, 95],\n",
       " [27, 1920, 25, 206, 95, 10],\n",
       " [27, 1920, 25, 206, 95, 10, 91],\n",
       " [27, 1920, 25, 206, 95, 10, 91, 1921],\n",
       " [27, 1920, 25, 206, 95, 10, 91, 1921, 1922],\n",
       " [25, 1923],\n",
       " [25, 1923, 1],\n",
       " [25, 1923, 1, 1924],\n",
       " [25, 1923, 1, 1924, 1925],\n",
       " [25, 1923, 1, 1924, 1925, 35],\n",
       " [25, 1923, 1, 1924, 1925, 35, 1],\n",
       " [25, 1923, 1, 1924, 1925, 35, 1, 1195],\n",
       " [71, 384],\n",
       " [140, 119],\n",
       " [140, 119, 690],\n",
       " [140, 119, 690, 225],\n",
       " [140, 119, 690, 225, 2],\n",
       " [140, 119, 690, 225, 2, 894],\n",
       " [140, 119, 690, 225, 2, 894, 58],\n",
       " [140, 119, 690, 225, 2, 894, 58, 16],\n",
       " [140, 119, 690, 225, 2, 894, 58, 16, 145],\n",
       " [140, 119, 690, 225, 2, 894, 58, 16, 145, 566],\n",
       " [17, 1926],\n",
       " [17, 1926, 1927],\n",
       " [17, 1926, 1927, 83],\n",
       " [17, 1926, 1927, 83, 25],\n",
       " [17, 1926, 1927, 83, 25, 277],\n",
       " [17, 1926, 1927, 83, 25, 277, 41],\n",
       " [17, 1926, 1927, 83, 25, 277, 41, 33],\n",
       " [17, 1926, 1927, 83, 25, 277, 41, 33, 257],\n",
       " [48, 10],\n",
       " [48, 10, 24],\n",
       " [48, 10, 24, 574],\n",
       " [48, 10, 24, 574, 261],\n",
       " [48, 10, 24, 574, 261, 3],\n",
       " [48, 10, 24, 574, 261, 3, 1928],\n",
       " [48, 10, 24, 574, 261, 3, 1928, 5],\n",
       " [48, 10, 24, 574, 261, 3, 1928, 5, 65],\n",
       " [48, 10, 24, 574, 261, 3, 1928, 5, 65, 14],\n",
       " [19, 10],\n",
       " [19, 10, 1],\n",
       " [19, 10, 1, 1196],\n",
       " [19, 10, 1, 1196, 2],\n",
       " [19, 10, 1, 1196, 2, 1197],\n",
       " [19, 10, 1, 1196, 2, 1197, 4],\n",
       " [19, 10, 1, 1196, 2, 1197, 4, 8],\n",
       " [19, 10, 1, 1196, 2, 1197, 4, 8, 1929],\n",
       " [16, 1930],\n",
       " [16, 1930, 106],\n",
       " [16, 1930, 106, 384],\n",
       " [16, 1930, 106, 384, 1931],\n",
       " [16, 1930, 106, 384, 1931, 3],\n",
       " [16, 1930, 106, 384, 1931, 3, 33],\n",
       " [16, 1930, 106, 384, 1931, 3, 33, 279],\n",
       " [140, 46],\n",
       " [140, 46, 51],\n",
       " [140, 46, 51, 411],\n",
       " [140, 46, 51, 411, 150],\n",
       " [140, 46, 51, 411, 150, 115],\n",
       " [140, 46, 51, 411, 150, 115, 21],\n",
       " [140, 46, 51, 411, 150, 115, 21, 25],\n",
       " [140, 46, 51, 411, 150, 115, 21, 25, 11],\n",
       " [140, 46, 51, 411, 150, 115, 21, 25, 11, 895],\n",
       " [77, 16],\n",
       " [77, 16, 275],\n",
       " [77, 16, 275, 1932],\n",
       " [77, 16, 275, 1932, 2],\n",
       " [77, 16, 275, 1932, 2, 62],\n",
       " [77, 16, 275, 1932, 2, 62, 1933],\n",
       " [77, 16, 275, 1932, 2, 62, 1933, 257],\n",
       " [27, 1198],\n",
       " [27, 1198, 1934],\n",
       " [27, 1198, 1934, 1],\n",
       " [27, 1198, 1934, 1, 896],\n",
       " [27, 1198, 1934, 1, 896, 4],\n",
       " [27, 1198, 1934, 1, 896, 4, 1],\n",
       " [27, 1198, 1934, 1, 896, 4, 1, 575],\n",
       " [2, 77],\n",
       " [2, 77, 93],\n",
       " [2, 77, 93, 1935],\n",
       " [2, 77, 93, 1935, 497],\n",
       " [2, 77, 93, 1935, 497, 4],\n",
       " [2, 77, 93, 1935, 497, 4, 1936],\n",
       " [2, 77, 93, 1935, 497, 4, 1936, 698],\n",
       " [2, 1937],\n",
       " [2, 1937, 1938],\n",
       " [2, 1937, 1938, 20],\n",
       " [2, 1937, 1938, 20, 1939],\n",
       " [2, 1937, 1938, 20, 1939, 4],\n",
       " [2, 1937, 1938, 20, 1939, 4, 1199],\n",
       " [77, 93],\n",
       " [77, 93, 1940],\n",
       " [77, 93, 1940, 4],\n",
       " [77, 93, 1940, 4, 1200],\n",
       " [77, 93, 1940, 4, 1200, 1941],\n",
       " [77, 93, 1940, 4, 1200, 1941, 155],\n",
       " [77, 93, 1940, 4, 1200, 1941, 155, 897],\n",
       " [77, 93, 1940, 4, 1200, 1941, 155, 897, 1942],\n",
       " [576, 14],\n",
       " [576, 14, 1943],\n",
       " [576, 14, 1943, 1],\n",
       " [576, 14, 1943, 1, 1944],\n",
       " [576, 14, 1943, 1, 1944, 56],\n",
       " [576, 14, 1943, 1, 1944, 56, 1],\n",
       " [576, 14, 1943, 1, 1944, 56, 1, 1945],\n",
       " [24, 146],\n",
       " [24, 146, 26],\n",
       " [24, 146, 26, 699],\n",
       " [24, 146, 26, 699, 11],\n",
       " [24, 146, 26, 699, 11, 16],\n",
       " [24, 146, 26, 699, 11, 16, 1946],\n",
       " [24, 146, 26, 699, 11, 16, 1946, 149],\n",
       " [156, 87],\n",
       " [156, 87, 1],\n",
       " [156, 87, 1, 124],\n",
       " [156, 87, 1, 124, 1201],\n",
       " [156, 87, 1, 124, 1201, 1947],\n",
       " [156, 87, 1, 124, 1201, 1947, 17],\n",
       " [156, 87, 1, 124, 1201, 1947, 17, 1],\n",
       " [156, 87, 1, 124, 1201, 1947, 17, 1, 169],\n",
       " [121, 280],\n",
       " [121, 280, 11],\n",
       " [121, 280, 11, 132],\n",
       " [121, 280, 11, 132, 1948],\n",
       " [121, 280, 11, 132, 1948, 21],\n",
       " [48, 11],\n",
       " [48, 11, 132],\n",
       " [48, 11, 132, 5],\n",
       " [58, 898],\n",
       " [58, 898, 1],\n",
       " [58, 898, 1, 1949],\n",
       " [58, 898, 1, 1949, 281],\n",
       " [58, 898, 1, 1949, 281, 27],\n",
       " [58, 898, 1, 1949, 281, 27, 33],\n",
       " [58, 898, 1, 1949, 281, 27, 33, 274],\n",
       " [58, 898, 1, 1949, 281, 27, 33, 274, 30],\n",
       " [155, 700],\n",
       " [155, 700, 184],\n",
       " [155, 700, 184, 19],\n",
       " [155, 700, 184, 19, 51],\n",
       " [155, 700, 184, 19, 51, 886],\n",
       " [155, 700, 184, 19, 51, 886, 3],\n",
       " [155, 700, 184, 19, 51, 886, 3, 72],\n",
       " [59, 23],\n",
       " [59, 23, 6],\n",
       " [59, 23, 6, 65],\n",
       " [59, 23, 6, 65, 41],\n",
       " [59, 23, 6, 65, 41, 282],\n",
       " [59, 23, 6, 65, 41, 282, 4],\n",
       " [59, 23, 6, 65, 41, 282, 4, 701],\n",
       " [1950, 1951],\n",
       " [1950, 1951, 35],\n",
       " [1950, 1951, 35, 41],\n",
       " [1950, 1951, 35, 41, 7],\n",
       " [1950, 1951, 35, 41, 7, 62],\n",
       " [1950, 1951, 35, 41, 7, 62, 1952],\n",
       " [1950, 1951, 35, 41, 7, 62, 1952, 1953],\n",
       " [1954, 3],\n",
       " [1954, 3, 1],\n",
       " [1954, 3, 1, 1955],\n",
       " [1954, 3, 1, 1955, 10],\n",
       " [1954, 3, 1, 1955, 10, 75],\n",
       " [1954, 3, 1, 1955, 10, 75, 33],\n",
       " [1954, 3, 1, 1955, 10, 75, 33, 899],\n",
       " [1954, 3, 1, 1955, 10, 75, 33, 899, 45],\n",
       " [20, 27],\n",
       " [20, 27, 16],\n",
       " [20, 27, 16, 1202],\n",
       " [20, 27, 16, 1202, 4],\n",
       " [20, 27, 16, 1202, 4, 33],\n",
       " [20, 27, 16, 1202, 4, 33, 454],\n",
       " [20, 27, 16, 1202, 4, 33, 454, 170],\n",
       " [20, 27, 16, 1202, 4, 33, 454, 170, 1956],\n",
       " [20, 27, 16, 1202, 4, 33, 454, 170, 1956, 28],\n",
       " [76, 1957],\n",
       " [76, 1957, 16],\n",
       " [76, 1957, 16, 282],\n",
       " [76, 1957, 16, 282, 121],\n",
       " [76, 1957, 16, 282, 121, 41],\n",
       " [76, 1957, 16, 282, 121, 41, 7],\n",
       " [76, 1957, 16, 282, 121, 41, 7, 702],\n",
       " [76, 1957, 16, 282, 121, 41, 7, 702, 1958],\n",
       " [64, 1959],\n",
       " [64, 1959, 41],\n",
       " [64, 1959, 41, 455],\n",
       " [64, 1959, 41, 455, 2],\n",
       " [64, 1959, 41, 455, 2, 1960],\n",
       " [76, 1961],\n",
       " [76, 1961, 17],\n",
       " [76, 1961, 17, 15],\n",
       " [76, 1961, 17, 15, 137],\n",
       " [76, 1961, 17, 15, 137, 38],\n",
       " [76, 1961, 17, 15, 137, 38, 194],\n",
       " [76, 1961, 17, 15, 137, 38, 194, 15],\n",
       " [76, 1961, 17, 15, 137, 38, 194, 15, 703],\n",
       " [75, 25],\n",
       " [75, 25, 704],\n",
       " [75, 25, 704, 1962],\n",
       " [75, 25, 704, 1962, 35],\n",
       " [75, 25, 704, 1962, 35, 3],\n",
       " [75, 25, 704, 1962, 35, 3, 1],\n",
       " [75, 25, 704, 1962, 35, 3, 1, 1963],\n",
       " [191, 1],\n",
       " [191, 1, 75],\n",
       " [191, 1, 75, 7],\n",
       " [191, 1, 75, 7, 1964],\n",
       " [191, 1, 75, 7, 1964, 1965],\n",
       " [59, 1966],\n",
       " [59, 1966, 41],\n",
       " [59, 1966, 41, 33],\n",
       " [59, 1966, 41, 33, 30],\n",
       " [59, 1966, 41, 33, 30, 75],\n",
       " [59, 1966, 41, 33, 30, 75, 108],\n",
       " [59, 1966, 41, 33, 30, 75, 108, 705],\n",
       " [3, 1],\n",
       " [3, 1, 1967],\n",
       " [3, 1, 1967, 4],\n",
       " [3, 1, 1967, 4, 282],\n",
       " [108, 25],\n",
       " [108, 25, 456],\n",
       " [108, 25, 456, 1968],\n",
       " [108, 25, 456, 1968, 23],\n",
       " [108, 25, 456, 1968, 23, 41],\n",
       " [108, 25, 456, 1968, 23, 41, 1],\n",
       " [108, 25, 456, 1968, 23, 41, 1, 275],\n",
       " [108, 25, 456, 1968, 23, 41, 1, 275, 1969],\n",
       " [2, 1970],\n",
       " [2, 1970, 4],\n",
       " [2, 1970, 4, 1],\n",
       " [2, 1970, 4, 1, 1971],\n",
       " [2, 1970, 4, 1, 1971, 1972],\n",
       " [15, 415],\n",
       " [15, 415, 3],\n",
       " [15, 415, 3, 45],\n",
       " [15, 415, 3, 45, 51],\n",
       " [15, 415, 3, 45, 51, 73],\n",
       " [15, 415, 3, 45, 51, 73, 385],\n",
       " [15, 415, 3, 45, 51, 73, 385, 282],\n",
       " [4, 1973],\n",
       " [4, 1973, 1203],\n",
       " [4, 1973, 1203, 498],\n",
       " [4, 1973, 1203, 498, 2],\n",
       " [4, 1973, 1203, 498, 2, 351],\n",
       " [83, 10],\n",
       " [83, 10, 1],\n",
       " [83, 10, 1, 1974],\n",
       " [83, 10, 1, 1974, 4],\n",
       " [83, 10, 1, 1974, 4, 701],\n",
       " [83, 10, 1, 1974, 4, 701, 107],\n",
       " [83, 10, 1, 1974, 4, 701, 107, 2],\n",
       " [83, 10, 1, 1974, 4, 701, 107, 2, 63],\n",
       " [1975, 128],\n",
       " [1975, 128, 7],\n",
       " [1975, 128, 7, 499],\n",
       " [1975, 128, 7, 499, 4],\n",
       " [1975, 128, 7, 499, 4, 1976],\n",
       " [1975, 128, 7, 499, 4, 1976, 1977],\n",
       " [20, 1978],\n",
       " [20, 1978, 2],\n",
       " [20, 1978, 2, 1204],\n",
       " [20, 1978, 2, 1204, 3],\n",
       " [20, 1978, 2, 1204, 3, 106],\n",
       " [20, 1978, 2, 1204, 3, 106, 1205],\n",
       " [11, 83],\n",
       " [11, 83, 7],\n",
       " [11, 83, 7, 1979],\n",
       " [11, 83, 7, 1979, 324],\n",
       " [11, 83, 7, 1979, 324, 75],\n",
       " [11, 83, 7, 1979, 324, 75, 13],\n",
       " [11, 83, 7, 1979, 324, 75, 13, 32],\n",
       " [11, 83, 7, 1979, 324, 75, 13, 32, 157],\n",
       " [2, 9],\n",
       " [2, 9, 156],\n",
       " [2, 9, 156, 64],\n",
       " [2, 9, 156, 64, 890],\n",
       " [2, 9, 156, 64, 890, 577],\n",
       " [2, 9, 156, 64, 890, 577, 33],\n",
       " [2, 9, 156, 64, 890, 577, 33, 279],\n",
       " [19, 3],\n",
       " [19, 3, 900],\n",
       " [19, 3, 900, 4],\n",
       " [19, 3, 900, 4, 72],\n",
       " [19, 3, 900, 4, 72, 41],\n",
       " [19, 3, 900, 4, 72, 41, 706],\n",
       " [19, 3, 900, 4, 72, 41, 706, 207],\n",
       " [2, 901],\n",
       " [2, 901, 1980],\n",
       " [2, 901, 1980, 194],\n",
       " [2, 901, 1980, 194, 1981],\n",
       " [2, 901, 1980, 194, 1981, 703],\n",
       " [27, 41],\n",
       " [27, 41, 15],\n",
       " [27, 41, 15, 97],\n",
       " [27, 41, 15, 97, 416],\n",
       " [27, 41, 15, 97, 416, 2],\n",
       " [27, 41, 15, 97, 416, 2, 16],\n",
       " [27, 41, 15, 97, 416, 2, 16, 5],\n",
       " [27, 41, 15, 97, 416, 2, 16, 5, 136],\n",
       " [27, 41, 15, 97, 416, 2, 16, 5, 136, 9],\n",
       " [13, 1],\n",
       " [13, 1, 902],\n",
       " [13, 1, 902, 707],\n",
       " [13, 1, 902, 707, 4],\n",
       " [13, 1, 902, 707, 4, 33],\n",
       " [13, 1, 902, 707, 4, 33, 1982],\n",
       " [1, 1206],\n",
       " [1, 1206, 4],\n",
       " [1, 1206, 4, 16],\n",
       " [1, 1206, 4, 16, 33],\n",
       " [1, 1206, 4, 16, 33, 257],\n",
       " [1, 1206, 4, 16, 33, 257, 2],\n",
       " [1, 1206, 4, 16, 33, 257, 2, 1],\n",
       " [1, 1206, 4, 16, 33, 257, 2, 1, 1207],\n",
       " [1, 1206, 4, 16, 33, 257, 2, 1, 1207, 164],\n",
       " [4, 16],\n",
       " [4, 16, 1208],\n",
       " [4, 16, 1208, 149],\n",
       " [4, 16, 1208, 149, 2],\n",
       " [4, 16, 1208, 149, 2, 1983],\n",
       " [4, 16, 1208, 149, 2, 1983, 10],\n",
       " [4, 16, 1208, 149, 2, 1983, 10, 1],\n",
       " [4, 16, 1208, 149, 2, 1983, 10, 1, 575],\n",
       " [57, 183],\n",
       " [57, 183, 131],\n",
       " [19, 417],\n",
       " [19, 417, 1984],\n",
       " [19, 417, 1984, 903],\n",
       " [19, 417, 1984, 903, 89],\n",
       " [19, 417, 1984, 903, 89, 9],\n",
       " [19, 417, 1984, 903, 89, 9, 173],\n",
       " [19, 417, 1984, 903, 89, 9, 173, 131],\n",
       " [81, 1209],\n",
       " [81, 1209, 9],\n",
       " [81, 1209, 9, 179],\n",
       " [81, 1209, 9, 179, 9],\n",
       " [81, 1209, 9, 179, 9, 1210],\n",
       " [81, 1209, 9, 179, 9, 1210, 21],\n",
       " [81, 1209, 9, 179, 9, 1210, 21, 323],\n",
       " [81, 1209, 9, 179, 9, 1210, 21, 323, 1985],\n",
       " [37, 42],\n",
       " [37, 42, 149],\n",
       " [37, 42, 149, 227],\n",
       " [37, 42, 149, 227, 457],\n",
       " [37, 42, 149, 227, 457, 44],\n",
       " [37, 42, 149, 227, 457, 44, 283],\n",
       " [37, 42, 149, 227, 457, 44, 283, 4],\n",
       " [37, 42, 149, 227, 457, 44, 283, 4, 352],\n",
       " [85, 3],\n",
       " [85, 3, 21],\n",
       " [85, 3, 21, 37],\n",
       " [85, 3, 21, 37, 63],\n",
       " [85, 3, 21, 37, 63, 26],\n",
       " [85, 3, 21, 37, 63, 26, 227],\n",
       " [85, 3, 21, 37, 63, 26, 227, 46],\n",
       " [85, 3, 21, 37, 63, 26, 227, 46, 167],\n",
       " [85, 3, 21, 37, 63, 26, 227, 46, 167, 3],\n",
       " [85, 3, 21, 37, 63, 26, 227, 46, 167, 3, 26],\n",
       " [85, 3, 21, 37, 63, 26, 227, 46, 167, 3, 26, 174],\n",
       " [11, 70],\n",
       " [11, 70, 3],\n",
       " [11, 70, 3, 80],\n",
       " [11, 70, 3, 80, 47],\n",
       " [11, 70, 3, 80, 47, 708],\n",
       " [11, 70, 3, 80, 47, 708, 2],\n",
       " [11, 70, 3, 80, 47, 708, 2, 284],\n",
       " [11, 70, 3, 80, 47, 708, 2, 284, 3],\n",
       " [11, 70, 3, 80, 47, 708, 2, 284, 3, 21],\n",
       " [11, 70, 3, 80, 47, 708, 2, 284, 3, 21, 578],\n",
       " [11, 70, 3, 80, 47, 708, 2, 284, 3, 21, 578, 3],\n",
       " [11, 70, 3, 80, 47, 708, 2, 284, 3, 21, 578, 3, 21],\n",
       " [37, 42],\n",
       " [37, 42, 238],\n",
       " [37, 42, 238, 1986],\n",
       " [37, 42, 238, 1986, 3],\n",
       " [37, 42, 238, 1986, 3, 52],\n",
       " [37, 42, 238, 1986, 3, 52, 1211],\n",
       " [37, 42, 238, 1986, 3, 52, 1211, 1212],\n",
       " [75, 1213],\n",
       " [75, 1213, 1987],\n",
       " [75, 1213, 1987, 70],\n",
       " [75, 1213, 1987, 70, 1214],\n",
       " [75, 1213, 1987, 70, 1214, 60],\n",
       " [75, 1213, 1987, 70, 1214, 60, 85],\n",
       " [44, 37],\n",
       " [44, 37, 42],\n",
       " [44, 37, 42, 149],\n",
       " [44, 37, 42, 149, 128],\n",
       " [44, 37, 42, 149, 128, 1988],\n",
       " [44, 37, 42, 149, 128, 1988, 10],\n",
       " [44, 37, 42, 149, 128, 1988, 10, 52],\n",
       " [44, 37, 42, 149, 128, 1988, 10, 52, 137],\n",
       " [1989, 709],\n",
       " [1989, 709, 10],\n",
       " [1989, 709, 10, 1],\n",
       " [1989, 709, 10, 1, 1990],\n",
       " [1989, 709, 10, 1, 1990, 4],\n",
       " [1989, 709, 10, 1, 1990, 4, 185],\n",
       " [20, 75],\n",
       " [20, 75, 55],\n",
       " [20, 75, 55, 94],\n",
       " [20, 75, 55, 94, 6],\n",
       " [20, 75, 55, 94, 6, 579],\n",
       " [20, 75, 55, 94, 6, 579, 353],\n",
       " [20, 75, 55, 94, 6, 579, 353, 386],\n",
       " [20, 75, 55, 94, 6, 579, 353, 386, 10],\n",
       " [20, 75, 55, 94, 6, 579, 353, 386, 10, 129],\n",
       " [85, 4],\n",
       " [85, 4, 9],\n",
       " [85, 4, 9, 323],\n",
       " [85, 4, 9, 323, 2],\n",
       " [85, 4, 9, 323, 2, 85],\n",
       " [85, 4, 9, 323, 2, 85, 710],\n",
       " [85, 4, 9, 323, 2, 85, 710, 9],\n",
       " [85, 4, 9, 323, 2, 85, 710, 9, 347],\n",
       " [140, 40],\n",
       " [140, 40, 5],\n",
       " [140, 40, 5, 1215],\n",
       " [140, 40, 5, 1215, 58],\n",
       " [140, 40, 5, 1215, 58, 9],\n",
       " [140, 40, 5, 1215, 58, 9, 17],\n",
       " [140, 40, 5, 1215, 58, 9, 17, 8],\n",
       " [140, 40, 5, 1215, 58, 9, 17, 8, 1991],\n",
       " [48, 47],\n",
       " [48, 47, 37],\n",
       " [48, 47, 37, 9],\n",
       " [48, 47, 37, 9, 31],\n",
       " [48, 47, 37, 9, 31, 14],\n",
       " [48, 47, 37, 9, 31, 14, 247],\n",
       " [346, 71],\n",
       " [346, 71, 107],\n",
       " [48, 71],\n",
       " [48, 71, 107],\n",
       " [140, 71],\n",
       " [140, 71, 277],\n",
       " [218, 183],\n",
       " [34, 47],\n",
       " [34, 47, 9],\n",
       " [34, 47, 9, 500],\n",
       " [34, 47, 9, 500, 325],\n",
       " [34, 47, 9, 500, 325, 27],\n",
       " [34, 47, 9, 500, 325, 27, 1216],\n",
       " [3, 904],\n",
       " [3, 904, 9],\n",
       " [3, 904, 9, 1],\n",
       " [3, 904, 9, 1, 208],\n",
       " [3, 904, 9, 1, 208, 4],\n",
       " [3, 904, 9, 1, 208, 4, 711],\n",
       " [20, 9],\n",
       " [20, 9, 13],\n",
       " [20, 9, 13, 23],\n",
       " [20, 9, 13, 23, 1],\n",
       " [20, 9, 13, 23, 1, 303],\n",
       " [20, 9, 13, 23, 1, 303, 1992],\n",
       " [2, 33],\n",
       " [2, 33, 1993],\n",
       " [2, 33, 1993, 1217],\n",
       " [2, 33, 1993, 1217, 1218],\n",
       " [2, 33, 1993, 1217, 1218, 1994],\n",
       " [346, 9],\n",
       " [346, 9, 59],\n",
       " [346, 9, 59, 209],\n",
       " [346, 9, 59, 209, 3],\n",
       " [346, 9, 59, 209, 3, 85],\n",
       " [346, 9, 59, 209, 3, 85, 95],\n",
       " [346, 9, 59, 209, 3, 85, 95, 1],\n",
       " [346, 9, 59, 209, 3, 85, 95, 1, 580],\n",
       " [346, 9, 59, 209, 3, 85, 95, 1, 580, 1219],\n",
       " [48, 2],\n",
       " [48, 2, 39],\n",
       " [48, 2, 39, 9],\n",
       " [48, 2, 39, 9, 1995],\n",
       " [48, 2, 39, 9, 1995, 61],\n",
       " [48, 2, 39, 9, 1995, 61, 7],\n",
       " [48, 2, 39, 9, 1995, 61, 7, 581],\n",
       " [48, 2, 39, 9, 1995, 61, 7, 581, 167],\n",
       " [96, 7],\n",
       " [96, 7, 1996],\n",
       " [96, 7, 1996, 1997],\n",
       " [96, 7, 1996, 1997, 5],\n",
       " [96, 7, 1996, 1997, 5, 29],\n",
       " [96, 7, 1996, 1997, 5, 29, 262],\n",
       " [1, 580],\n",
       " [1, 580, 11],\n",
       " [1, 580, 11, 13],\n",
       " [1, 580, 11, 13, 1],\n",
       " [1, 580, 11, 13, 1, 712],\n",
       " [1, 580, 11, 13, 1, 712, 3],\n",
       " [1, 580, 11, 13, 1, 712, 3, 1],\n",
       " [1, 580, 11, 13, 1, 712, 3, 1, 169],\n",
       " [156, 17],\n",
       " [156, 17, 15],\n",
       " [156, 17, 15, 1998],\n",
       " [156, 17, 15, 1998, 2],\n",
       " [156, 17, 15, 1998, 2, 1999],\n",
       " [156, 17, 15, 1998, 2, 1999, 2000],\n",
       " [156, 17, 15, 1998, 2, 1999, 2000, 1220],\n",
       " [1221, 1],\n",
       " [1221, 1, 163],\n",
       " [1221, 1, 163, 4],\n",
       " [1221, 1, 163, 4, 169],\n",
       " [1221, 1, 163, 4, 169, 2],\n",
       " [1221, 1, 163, 4, 169, 2, 58],\n",
       " [1221, 1, 163, 4, 169, 2, 58, 15],\n",
       " [1221, 1, 163, 4, 169, 2, 58, 15, 2001],\n",
       " [713, 10],\n",
       " [713, 10, 501],\n",
       " [713, 10, 501, 44],\n",
       " [713, 10, 501, 44, 326],\n",
       " [713, 10, 501, 44, 326, 10],\n",
       " [713, 10, 501, 44, 326, 10, 185],\n",
       " [713, 10, 501, 44, 326, 10, 185, 44],\n",
       " [713, 10, 501, 44, 326, 10, 185, 44, 303],\n",
       " [134, 2002],\n",
       " [134, 2002, 2],\n",
       " [134, 2002, 2, 2003],\n",
       " [134, 2002, 2, 2003, 263],\n",
       " [134, 2002, 2, 2003, 263, 2004],\n",
       " [3, 15],\n",
       " [3, 15, 1222],\n",
       " [3, 15, 1222, 2],\n",
       " [3, 15, 1222, 2, 4],\n",
       " [3, 15, 1222, 2, 4, 1],\n",
       " [3, 15, 1222, 2, 4, 1, 502],\n",
       " [3, 15, 1222, 2, 4, 1, 502, 714],\n",
       " [16, 905],\n",
       " [16, 905, 2005],\n",
       " [16, 905, 2005, 147],\n",
       " [16, 905, 2005, 147, 2006],\n",
       " [140, 9],\n",
       " [140, 9, 2007],\n",
       " [140, 9, 2007, 35],\n",
       " [140, 9, 2007, 35, 1],\n",
       " [140, 9, 2007, 35, 1, 2008],\n",
       " [140, 9, 2007, 35, 1, 2008, 4],\n",
       " [140, 9, 2007, 35, 1, 2008, 4, 1],\n",
       " [140, 9, 2007, 35, 1, 2008, 4, 1, 580],\n",
       " [106, 503],\n",
       " [106, 503, 11],\n",
       " [106, 503, 11, 285],\n",
       " [106, 503, 11, 285, 582],\n",
       " [106, 503, 11, 285, 582, 11],\n",
       " [106, 503, 11, 285, 582, 11, 504],\n",
       " [106, 503, 11, 285, 582, 11, 504, 173],\n",
       " [715, 33],\n",
       " [715, 33, 2009],\n",
       " [715, 33, 2009, 2010],\n",
       " [715, 33, 2009, 2010, 13],\n",
       " [715, 33, 2009, 2010, 13, 2011],\n",
       " [1, 1223],\n",
       " [1, 1223, 4],\n",
       " [1, 1223, 4, 2012],\n",
       " [1, 1223, 4, 2012, 2013],\n",
       " [1, 1223, 4, 2012, 2013, 38],\n",
       " [1, 1223, 4, 2012, 2013, 38, 124],\n",
       " [1, 1223, 4, 2012, 2013, 38, 124, 224],\n",
       " [2, 39],\n",
       " [2, 39, 55],\n",
       " [2, 39, 55, 94],\n",
       " [2, 39, 55, 94, 32],\n",
       " [2, 39, 55, 94, 32, 263],\n",
       " [2, 39, 55, 94, 32, 263, 132],\n",
       " [2, 39, 55, 94, 32, 263, 132, 386],\n",
       " [2, 39, 55, 94, 32, 263, 132, 386, 2014],\n",
       " [1, 692],\n",
       " [1, 692, 36],\n",
       " [1, 692, 36, 583],\n",
       " [1, 692, 36, 583, 39],\n",
       " [1, 692, 36, 583, 39, 32],\n",
       " [1, 692, 36, 583, 39, 32, 2015],\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create input sequences\n",
    "input_sequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[: i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad sequence\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    1,  687],\n",
       "       [   0,    0,    0, ...,    1,  687,    4],\n",
       "       [   0,    0,    0, ...,  687,    4,   45],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    4,   45, 1047],\n",
       "       [   0,    0,    0, ...,   45, 1047,    4],\n",
       "       [   0,    0,    0, ..., 1047,    4,  193]], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictors and labels\n",
    "import tensorflow as tf\n",
    "x, y = input_sequences[:,:-1], input_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    0,    1],\n",
       "       [   0,    0,    0, ...,    0,    1,  687],\n",
       "       [   0,    0,    0, ...,    1,  687,    4],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  687,    4,   45],\n",
       "       [   0,    0,    0, ...,    4,   45, 1047],\n",
       "       [   0,    0,    0, ...,   45, 1047,    4]], dtype=int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 687,    4,   45, ..., 1047,    4,  193], dtype=int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the y label into categorical\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 13, 100)           481800    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 13, 150)           150600    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 13, 150)           0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               100400    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4818)              486618    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1219418 (4.65 MB)\n",
      "Trainable params: 1219418 (4.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Train our LSTM RNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length = max_sequence_len-1))\n",
    "model.add(LSTM(150, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation ='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "644/644 [==============================] - 10s 12ms/step - loss: 6.9125 - accuracy: 0.0316 - val_loss: 6.7532 - val_accuracy: 0.0365\n",
      "Epoch 2/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 6.4699 - accuracy: 0.0377 - val_loss: 6.8300 - val_accuracy: 0.0437\n",
      "Epoch 3/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 6.3252 - accuracy: 0.0465 - val_loss: 6.8921 - val_accuracy: 0.0544\n",
      "Epoch 4/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 6.1828 - accuracy: 0.0526 - val_loss: 6.9277 - val_accuracy: 0.0548\n",
      "Epoch 5/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 6.0412 - accuracy: 0.0576 - val_loss: 6.9341 - val_accuracy: 0.0579\n",
      "Epoch 6/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 5.8952 - accuracy: 0.0655 - val_loss: 6.9880 - val_accuracy: 0.0622\n",
      "Epoch 7/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 5.7527 - accuracy: 0.0740 - val_loss: 7.0309 - val_accuracy: 0.0620\n",
      "Epoch 8/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 5.6267 - accuracy: 0.0795 - val_loss: 7.0992 - val_accuracy: 0.0635\n",
      "Epoch 9/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 5.5093 - accuracy: 0.0837 - val_loss: 7.1461 - val_accuracy: 0.0680\n",
      "Epoch 10/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 5.3968 - accuracy: 0.0899 - val_loss: 7.2589 - val_accuracy: 0.0663\n",
      "Epoch 11/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 5.2860 - accuracy: 0.0971 - val_loss: 7.3534 - val_accuracy: 0.0672\n",
      "Epoch 12/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 5.1773 - accuracy: 0.1012 - val_loss: 7.4436 - val_accuracy: 0.0672\n",
      "Epoch 13/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 5.0677 - accuracy: 0.1061 - val_loss: 7.5508 - val_accuracy: 0.0676\n",
      "Epoch 14/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 4.9593 - accuracy: 0.1104 - val_loss: 7.6258 - val_accuracy: 0.0672\n",
      "Epoch 15/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 4.8519 - accuracy: 0.1146 - val_loss: 7.7840 - val_accuracy: 0.0680\n",
      "Epoch 16/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 4.7466 - accuracy: 0.1194 - val_loss: 7.9211 - val_accuracy: 0.0666\n",
      "Epoch 17/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 4.6370 - accuracy: 0.1253 - val_loss: 8.0145 - val_accuracy: 0.0672\n",
      "Epoch 18/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 4.5353 - accuracy: 0.1298 - val_loss: 8.1941 - val_accuracy: 0.0692\n",
      "Epoch 19/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 4.4352 - accuracy: 0.1375 - val_loss: 8.2827 - val_accuracy: 0.0639\n",
      "Epoch 20/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 4.3359 - accuracy: 0.1448 - val_loss: 8.3839 - val_accuracy: 0.0655\n",
      "Epoch 21/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 4.2404 - accuracy: 0.1528 - val_loss: 8.6028 - val_accuracy: 0.0709\n",
      "Epoch 22/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 4.1502 - accuracy: 0.1652 - val_loss: 8.6756 - val_accuracy: 0.0657\n",
      "Epoch 23/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 4.0676 - accuracy: 0.1772 - val_loss: 8.8309 - val_accuracy: 0.0624\n",
      "Epoch 24/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 3.9837 - accuracy: 0.1871 - val_loss: 8.9978 - val_accuracy: 0.0643\n",
      "Epoch 25/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 3.9063 - accuracy: 0.1973 - val_loss: 9.0768 - val_accuracy: 0.0637\n",
      "Epoch 26/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 3.8302 - accuracy: 0.2103 - val_loss: 9.2210 - val_accuracy: 0.0641\n",
      "Epoch 27/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 3.7653 - accuracy: 0.2221 - val_loss: 9.3438 - val_accuracy: 0.0616\n",
      "Epoch 28/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 3.6918 - accuracy: 0.2353 - val_loss: 9.4438 - val_accuracy: 0.0629\n",
      "Epoch 29/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 3.6275 - accuracy: 0.2454 - val_loss: 9.5913 - val_accuracy: 0.0606\n",
      "Epoch 30/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 3.5673 - accuracy: 0.2542 - val_loss: 9.6933 - val_accuracy: 0.0643\n",
      "Epoch 31/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 3.5066 - accuracy: 0.2632 - val_loss: 9.7862 - val_accuracy: 0.0598\n",
      "Epoch 32/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 3.4547 - accuracy: 0.2757 - val_loss: 9.8899 - val_accuracy: 0.0647\n",
      "Epoch 33/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 3.3944 - accuracy: 0.2836 - val_loss: 9.9382 - val_accuracy: 0.0649\n",
      "Epoch 34/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 3.3418 - accuracy: 0.2937 - val_loss: 10.0663 - val_accuracy: 0.0614\n",
      "Epoch 35/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 3.2918 - accuracy: 0.2999 - val_loss: 10.1635 - val_accuracy: 0.0622\n",
      "Epoch 36/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 3.2383 - accuracy: 0.3111 - val_loss: 10.2983 - val_accuracy: 0.0593\n",
      "Epoch 37/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 3.1890 - accuracy: 0.3194 - val_loss: 10.3642 - val_accuracy: 0.0616\n",
      "Epoch 38/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 3.1408 - accuracy: 0.3274 - val_loss: 10.4749 - val_accuracy: 0.0616\n",
      "Epoch 39/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 3.0965 - accuracy: 0.3366 - val_loss: 10.5465 - val_accuracy: 0.0606\n",
      "Epoch 40/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 3.0548 - accuracy: 0.3401 - val_loss: 10.5842 - val_accuracy: 0.0622\n",
      "Epoch 41/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 3.0140 - accuracy: 0.3486 - val_loss: 10.6846 - val_accuracy: 0.0608\n",
      "Epoch 42/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.9760 - accuracy: 0.3537 - val_loss: 10.7338 - val_accuracy: 0.0577\n",
      "Epoch 43/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.9324 - accuracy: 0.3641 - val_loss: 10.8288 - val_accuracy: 0.0587\n",
      "Epoch 44/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.8847 - accuracy: 0.3750 - val_loss: 10.8767 - val_accuracy: 0.0587\n",
      "Epoch 45/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.8488 - accuracy: 0.3807 - val_loss: 10.9862 - val_accuracy: 0.0595\n",
      "Epoch 46/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.8159 - accuracy: 0.3866 - val_loss: 11.0420 - val_accuracy: 0.0608\n",
      "Epoch 47/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.7793 - accuracy: 0.3968 - val_loss: 11.1392 - val_accuracy: 0.0610\n",
      "Epoch 48/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.7443 - accuracy: 0.3982 - val_loss: 11.1961 - val_accuracy: 0.0614\n",
      "Epoch 49/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.7054 - accuracy: 0.4068 - val_loss: 11.2671 - val_accuracy: 0.0587\n",
      "Epoch 50/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.6740 - accuracy: 0.4165 - val_loss: 11.2983 - val_accuracy: 0.0585\n",
      "Epoch 51/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.6454 - accuracy: 0.4167 - val_loss: 11.3785 - val_accuracy: 0.0583\n",
      "Epoch 52/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.6113 - accuracy: 0.4239 - val_loss: 11.4763 - val_accuracy: 0.0596\n",
      "Epoch 53/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.5744 - accuracy: 0.4350 - val_loss: 11.5256 - val_accuracy: 0.0591\n",
      "Epoch 54/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.5459 - accuracy: 0.4385 - val_loss: 11.5744 - val_accuracy: 0.0600\n",
      "Epoch 55/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.5136 - accuracy: 0.4463 - val_loss: 11.6155 - val_accuracy: 0.0552\n",
      "Epoch 56/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.4882 - accuracy: 0.4472 - val_loss: 11.7106 - val_accuracy: 0.0579\n",
      "Epoch 57/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.4562 - accuracy: 0.4577 - val_loss: 11.7659 - val_accuracy: 0.0558\n",
      "Epoch 58/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.4277 - accuracy: 0.4625 - val_loss: 11.8266 - val_accuracy: 0.0532\n",
      "Epoch 59/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.4067 - accuracy: 0.4625 - val_loss: 11.8903 - val_accuracy: 0.0554\n",
      "Epoch 60/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.3731 - accuracy: 0.4714 - val_loss: 11.9436 - val_accuracy: 0.0577\n",
      "Epoch 61/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.3495 - accuracy: 0.4753 - val_loss: 11.9929 - val_accuracy: 0.0560\n",
      "Epoch 62/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 2.3187 - accuracy: 0.4847 - val_loss: 12.0701 - val_accuracy: 0.0554\n",
      "Epoch 63/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 2.2945 - accuracy: 0.4889 - val_loss: 12.1678 - val_accuracy: 0.0556\n",
      "Epoch 64/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.2686 - accuracy: 0.4941 - val_loss: 12.1648 - val_accuracy: 0.0558\n",
      "Epoch 65/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 2.2400 - accuracy: 0.4990 - val_loss: 12.2422 - val_accuracy: 0.0544\n",
      "Epoch 66/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.2174 - accuracy: 0.5023 - val_loss: 12.2666 - val_accuracy: 0.0546\n",
      "Epoch 67/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.1920 - accuracy: 0.5112 - val_loss: 12.3345 - val_accuracy: 0.0534\n",
      "Epoch 68/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.1760 - accuracy: 0.5108 - val_loss: 12.4011 - val_accuracy: 0.0563\n",
      "Epoch 69/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 2.1503 - accuracy: 0.5173 - val_loss: 12.4788 - val_accuracy: 0.0577\n",
      "Epoch 70/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.1280 - accuracy: 0.5208 - val_loss: 12.5548 - val_accuracy: 0.0550\n",
      "Epoch 71/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.1032 - accuracy: 0.5267 - val_loss: 12.5536 - val_accuracy: 0.0560\n",
      "Epoch 72/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 2.0769 - accuracy: 0.5327 - val_loss: 12.6451 - val_accuracy: 0.0558\n",
      "Epoch 73/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.0627 - accuracy: 0.5340 - val_loss: 12.6511 - val_accuracy: 0.0560\n",
      "Epoch 74/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 2.0315 - accuracy: 0.5402 - val_loss: 12.7372 - val_accuracy: 0.0560\n",
      "Epoch 75/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 2.0198 - accuracy: 0.5415 - val_loss: 12.7964 - val_accuracy: 0.0579\n",
      "Epoch 76/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.9979 - accuracy: 0.5450 - val_loss: 12.8240 - val_accuracy: 0.0544\n",
      "Epoch 77/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.9717 - accuracy: 0.5532 - val_loss: 12.8602 - val_accuracy: 0.0567\n",
      "Epoch 78/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.9623 - accuracy: 0.5549 - val_loss: 12.9149 - val_accuracy: 0.0554\n",
      "Epoch 79/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.9330 - accuracy: 0.5611 - val_loss: 12.9546 - val_accuracy: 0.0534\n",
      "Epoch 80/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.9239 - accuracy: 0.5631 - val_loss: 13.0156 - val_accuracy: 0.0530\n",
      "Epoch 81/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.8990 - accuracy: 0.5688 - val_loss: 13.0724 - val_accuracy: 0.0563\n",
      "Epoch 82/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.8745 - accuracy: 0.5709 - val_loss: 13.1285 - val_accuracy: 0.0536\n",
      "Epoch 83/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 1.8679 - accuracy: 0.5739 - val_loss: 13.1217 - val_accuracy: 0.0554\n",
      "Epoch 84/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 1.8445 - accuracy: 0.5789 - val_loss: 13.1981 - val_accuracy: 0.0515\n",
      "Epoch 85/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.8252 - accuracy: 0.5827 - val_loss: 13.2558 - val_accuracy: 0.0536\n",
      "Epoch 86/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.8102 - accuracy: 0.5856 - val_loss: 13.3102 - val_accuracy: 0.0532\n",
      "Epoch 87/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.7916 - accuracy: 0.5887 - val_loss: 13.3691 - val_accuracy: 0.0542\n",
      "Epoch 88/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.7685 - accuracy: 0.5952 - val_loss: 13.4100 - val_accuracy: 0.0538\n",
      "Epoch 89/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.7593 - accuracy: 0.5970 - val_loss: 13.4296 - val_accuracy: 0.0552\n",
      "Epoch 90/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.7405 - accuracy: 0.5982 - val_loss: 13.4623 - val_accuracy: 0.0530\n",
      "Epoch 91/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.7203 - accuracy: 0.6038 - val_loss: 13.5287 - val_accuracy: 0.0536\n",
      "Epoch 92/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 1.7009 - accuracy: 0.6078 - val_loss: 13.5851 - val_accuracy: 0.0530\n",
      "Epoch 93/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.6849 - accuracy: 0.6139 - val_loss: 13.6347 - val_accuracy: 0.0523\n",
      "Epoch 94/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.6726 - accuracy: 0.6170 - val_loss: 13.6599 - val_accuracy: 0.0525\n",
      "Epoch 95/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.6556 - accuracy: 0.6178 - val_loss: 13.6837 - val_accuracy: 0.0536\n",
      "Epoch 96/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.6400 - accuracy: 0.6210 - val_loss: 13.7679 - val_accuracy: 0.0525\n",
      "Epoch 97/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.6311 - accuracy: 0.6262 - val_loss: 13.8329 - val_accuracy: 0.0527\n",
      "Epoch 98/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.6163 - accuracy: 0.6277 - val_loss: 13.8451 - val_accuracy: 0.0513\n",
      "Epoch 99/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.5966 - accuracy: 0.6304 - val_loss: 13.9170 - val_accuracy: 0.0525\n",
      "Epoch 100/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.5813 - accuracy: 0.6358 - val_loss: 13.9657 - val_accuracy: 0.0534\n",
      "Epoch 101/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.5727 - accuracy: 0.6350 - val_loss: 13.9827 - val_accuracy: 0.0534\n",
      "Epoch 102/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.5546 - accuracy: 0.6395 - val_loss: 14.0307 - val_accuracy: 0.0486\n",
      "Epoch 103/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 1.5496 - accuracy: 0.6400 - val_loss: 14.0789 - val_accuracy: 0.0527\n",
      "Epoch 104/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.5327 - accuracy: 0.6410 - val_loss: 14.0944 - val_accuracy: 0.0462\n",
      "Epoch 105/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.5147 - accuracy: 0.6475 - val_loss: 14.1411 - val_accuracy: 0.0499\n",
      "Epoch 106/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.5127 - accuracy: 0.6477 - val_loss: 14.1727 - val_accuracy: 0.0507\n",
      "Epoch 107/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.4908 - accuracy: 0.6541 - val_loss: 14.2260 - val_accuracy: 0.0527\n",
      "Epoch 108/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.4803 - accuracy: 0.6548 - val_loss: 14.2979 - val_accuracy: 0.0503\n",
      "Epoch 109/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.4649 - accuracy: 0.6606 - val_loss: 14.3591 - val_accuracy: 0.0521\n",
      "Epoch 110/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.4586 - accuracy: 0.6636 - val_loss: 14.3750 - val_accuracy: 0.0505\n",
      "Epoch 111/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.4446 - accuracy: 0.6642 - val_loss: 14.3802 - val_accuracy: 0.0501\n",
      "Epoch 112/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.4295 - accuracy: 0.6656 - val_loss: 14.4105 - val_accuracy: 0.0515\n",
      "Epoch 113/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.4196 - accuracy: 0.6707 - val_loss: 14.4804 - val_accuracy: 0.0511\n",
      "Epoch 114/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.4046 - accuracy: 0.6749 - val_loss: 14.4956 - val_accuracy: 0.0492\n",
      "Epoch 115/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.3903 - accuracy: 0.6762 - val_loss: 14.5567 - val_accuracy: 0.0515\n",
      "Epoch 116/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.3835 - accuracy: 0.6791 - val_loss: 14.5496 - val_accuracy: 0.0505\n",
      "Epoch 117/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.3728 - accuracy: 0.6807 - val_loss: 14.6231 - val_accuracy: 0.0499\n",
      "Epoch 118/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.3657 - accuracy: 0.6790 - val_loss: 14.6588 - val_accuracy: 0.0505\n",
      "Epoch 119/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.3512 - accuracy: 0.6826 - val_loss: 14.7340 - val_accuracy: 0.0480\n",
      "Epoch 120/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.3422 - accuracy: 0.6873 - val_loss: 14.7395 - val_accuracy: 0.0492\n",
      "Epoch 121/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.3371 - accuracy: 0.6866 - val_loss: 14.7420 - val_accuracy: 0.0525\n",
      "Epoch 122/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.3203 - accuracy: 0.6905 - val_loss: 14.7998 - val_accuracy: 0.0484\n",
      "Epoch 123/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.3151 - accuracy: 0.6928 - val_loss: 14.8253 - val_accuracy: 0.0484\n",
      "Epoch 124/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.3032 - accuracy: 0.6961 - val_loss: 14.8636 - val_accuracy: 0.0515\n",
      "Epoch 125/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.2916 - accuracy: 0.6989 - val_loss: 14.8938 - val_accuracy: 0.0497\n",
      "Epoch 126/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.2863 - accuracy: 0.6967 - val_loss: 14.9583 - val_accuracy: 0.0472\n",
      "Epoch 127/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.2697 - accuracy: 0.7016 - val_loss: 14.9533 - val_accuracy: 0.0480\n",
      "Epoch 128/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.2686 - accuracy: 0.7026 - val_loss: 15.0001 - val_accuracy: 0.0490\n",
      "Epoch 129/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.2504 - accuracy: 0.7044 - val_loss: 15.0399 - val_accuracy: 0.0488\n",
      "Epoch 130/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.2416 - accuracy: 0.7077 - val_loss: 15.1044 - val_accuracy: 0.0497\n",
      "Epoch 131/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 1.2341 - accuracy: 0.7084 - val_loss: 15.1120 - val_accuracy: 0.0495\n",
      "Epoch 132/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.2315 - accuracy: 0.7096 - val_loss: 15.0947 - val_accuracy: 0.0521\n",
      "Epoch 133/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.2215 - accuracy: 0.7126 - val_loss: 15.1835 - val_accuracy: 0.0513\n",
      "Epoch 134/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 1.2184 - accuracy: 0.7130 - val_loss: 15.2176 - val_accuracy: 0.0513\n",
      "Epoch 135/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 1.2010 - accuracy: 0.7170 - val_loss: 15.1840 - val_accuracy: 0.0478\n",
      "Epoch 136/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 1.1963 - accuracy: 0.7174 - val_loss: 15.2582 - val_accuracy: 0.0484\n",
      "Epoch 137/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.1912 - accuracy: 0.7162 - val_loss: 15.2705 - val_accuracy: 0.0478\n",
      "Epoch 138/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 1.1784 - accuracy: 0.7203 - val_loss: 15.3763 - val_accuracy: 0.0493\n",
      "Epoch 139/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.1717 - accuracy: 0.7243 - val_loss: 15.3703 - val_accuracy: 0.0488\n",
      "Epoch 140/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.1639 - accuracy: 0.7261 - val_loss: 15.4361 - val_accuracy: 0.0453\n",
      "Epoch 141/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.1544 - accuracy: 0.7257 - val_loss: 15.4894 - val_accuracy: 0.0497\n",
      "Epoch 142/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.1452 - accuracy: 0.7299 - val_loss: 15.5175 - val_accuracy: 0.0503\n",
      "Epoch 143/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.1440 - accuracy: 0.7304 - val_loss: 15.5113 - val_accuracy: 0.0462\n",
      "Epoch 144/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.1341 - accuracy: 0.7324 - val_loss: 15.5614 - val_accuracy: 0.0480\n",
      "Epoch 145/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.1249 - accuracy: 0.7329 - val_loss: 15.5923 - val_accuracy: 0.0507\n",
      "Epoch 146/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.1140 - accuracy: 0.7356 - val_loss: 15.6400 - val_accuracy: 0.0486\n",
      "Epoch 147/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.1128 - accuracy: 0.7366 - val_loss: 15.6553 - val_accuracy: 0.0486\n",
      "Epoch 148/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.1088 - accuracy: 0.7350 - val_loss: 15.6786 - val_accuracy: 0.0462\n",
      "Epoch 149/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.0988 - accuracy: 0.7388 - val_loss: 15.7299 - val_accuracy: 0.0466\n",
      "Epoch 150/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.1020 - accuracy: 0.7371 - val_loss: 15.7374 - val_accuracy: 0.0464\n",
      "Epoch 151/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 1.0892 - accuracy: 0.7401 - val_loss: 15.7357 - val_accuracy: 0.0490\n",
      "Epoch 152/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.0936 - accuracy: 0.7381 - val_loss: 15.7776 - val_accuracy: 0.0466\n",
      "Epoch 153/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.0765 - accuracy: 0.7419 - val_loss: 15.8333 - val_accuracy: 0.0478\n",
      "Epoch 154/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.0690 - accuracy: 0.7434 - val_loss: 15.8129 - val_accuracy: 0.0480\n",
      "Epoch 155/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.0604 - accuracy: 0.7453 - val_loss: 15.8652 - val_accuracy: 0.0488\n",
      "Epoch 156/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.0514 - accuracy: 0.7486 - val_loss: 15.9139 - val_accuracy: 0.0515\n",
      "Epoch 157/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.0580 - accuracy: 0.7490 - val_loss: 15.9618 - val_accuracy: 0.0505\n",
      "Epoch 158/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.0469 - accuracy: 0.7490 - val_loss: 15.9593 - val_accuracy: 0.0490\n",
      "Epoch 159/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.0360 - accuracy: 0.7521 - val_loss: 15.9797 - val_accuracy: 0.0464\n",
      "Epoch 160/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 1.0224 - accuracy: 0.7538 - val_loss: 16.0386 - val_accuracy: 0.0484\n",
      "Epoch 161/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.0268 - accuracy: 0.7538 - val_loss: 16.0359 - val_accuracy: 0.0480\n",
      "Epoch 162/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 1.0194 - accuracy: 0.7515 - val_loss: 16.0909 - val_accuracy: 0.0472\n",
      "Epoch 163/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.0262 - accuracy: 0.7526 - val_loss: 16.1268 - val_accuracy: 0.0486\n",
      "Epoch 164/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.0123 - accuracy: 0.7566 - val_loss: 16.1210 - val_accuracy: 0.0505\n",
      "Epoch 165/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 1.0130 - accuracy: 0.7562 - val_loss: 16.1780 - val_accuracy: 0.0492\n",
      "Epoch 166/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 1.0001 - accuracy: 0.7588 - val_loss: 16.2117 - val_accuracy: 0.0486\n",
      "Epoch 167/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.9907 - accuracy: 0.7620 - val_loss: 16.2443 - val_accuracy: 0.0472\n",
      "Epoch 168/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.9876 - accuracy: 0.7618 - val_loss: 16.3085 - val_accuracy: 0.0457\n",
      "Epoch 169/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.9863 - accuracy: 0.7635 - val_loss: 16.3054 - val_accuracy: 0.0499\n",
      "Epoch 170/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.9842 - accuracy: 0.7635 - val_loss: 16.2926 - val_accuracy: 0.0484\n",
      "Epoch 171/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 0.9786 - accuracy: 0.7643 - val_loss: 16.3454 - val_accuracy: 0.0470\n",
      "Epoch 172/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 0.9734 - accuracy: 0.7664 - val_loss: 16.4266 - val_accuracy: 0.0482\n",
      "Epoch 173/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.9645 - accuracy: 0.7666 - val_loss: 16.3877 - val_accuracy: 0.0484\n",
      "Epoch 174/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 0.9615 - accuracy: 0.7692 - val_loss: 16.4508 - val_accuracy: 0.0503\n",
      "Epoch 175/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 0.9642 - accuracy: 0.7654 - val_loss: 16.4386 - val_accuracy: 0.0474\n",
      "Epoch 176/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 0.9559 - accuracy: 0.7687 - val_loss: 16.4823 - val_accuracy: 0.0480\n",
      "Epoch 177/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.9526 - accuracy: 0.7700 - val_loss: 16.5363 - val_accuracy: 0.0486\n",
      "Epoch 178/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.9461 - accuracy: 0.7702 - val_loss: 16.5395 - val_accuracy: 0.0460\n",
      "Epoch 179/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 0.9450 - accuracy: 0.7696 - val_loss: 16.5443 - val_accuracy: 0.0468\n",
      "Epoch 180/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 0.9383 - accuracy: 0.7700 - val_loss: 16.5795 - val_accuracy: 0.0464\n",
      "Epoch 181/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 0.9343 - accuracy: 0.7735 - val_loss: 16.5942 - val_accuracy: 0.0478\n",
      "Epoch 182/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 0.9277 - accuracy: 0.7737 - val_loss: 16.5932 - val_accuracy: 0.0464\n",
      "Epoch 183/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.9272 - accuracy: 0.7733 - val_loss: 16.6398 - val_accuracy: 0.0462\n",
      "Epoch 184/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.9307 - accuracy: 0.7711 - val_loss: 16.6595 - val_accuracy: 0.0453\n",
      "Epoch 185/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 0.9192 - accuracy: 0.7763 - val_loss: 16.7239 - val_accuracy: 0.0459\n",
      "Epoch 186/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.9167 - accuracy: 0.7768 - val_loss: 16.7322 - val_accuracy: 0.0460\n",
      "Epoch 187/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.9154 - accuracy: 0.7746 - val_loss: 16.7120 - val_accuracy: 0.0492\n",
      "Epoch 188/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.9153 - accuracy: 0.7776 - val_loss: 16.7486 - val_accuracy: 0.0451\n",
      "Epoch 189/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.9077 - accuracy: 0.7776 - val_loss: 16.7738 - val_accuracy: 0.0474\n",
      "Epoch 190/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.9015 - accuracy: 0.7800 - val_loss: 16.8034 - val_accuracy: 0.0447\n",
      "Epoch 191/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.8975 - accuracy: 0.7784 - val_loss: 16.8789 - val_accuracy: 0.0470\n",
      "Epoch 192/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 0.8949 - accuracy: 0.7812 - val_loss: 16.8132 - val_accuracy: 0.0486\n",
      "Epoch 193/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.8911 - accuracy: 0.7813 - val_loss: 16.8721 - val_accuracy: 0.0460\n",
      "Epoch 194/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 0.8997 - accuracy: 0.7782 - val_loss: 16.9241 - val_accuracy: 0.0468\n",
      "Epoch 195/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.8880 - accuracy: 0.7830 - val_loss: 16.9509 - val_accuracy: 0.0470\n",
      "Epoch 196/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8788 - accuracy: 0.7855 - val_loss: 17.0002 - val_accuracy: 0.0451\n",
      "Epoch 197/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8762 - accuracy: 0.7877 - val_loss: 17.0053 - val_accuracy: 0.0472\n",
      "Epoch 198/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8747 - accuracy: 0.7852 - val_loss: 17.0501 - val_accuracy: 0.0488\n",
      "Epoch 199/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8676 - accuracy: 0.7851 - val_loss: 17.0699 - val_accuracy: 0.0474\n",
      "Epoch 200/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8717 - accuracy: 0.7872 - val_loss: 16.9878 - val_accuracy: 0.0488\n",
      "Epoch 201/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8624 - accuracy: 0.7876 - val_loss: 17.1605 - val_accuracy: 0.0474\n",
      "Epoch 202/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8597 - accuracy: 0.7868 - val_loss: 17.1112 - val_accuracy: 0.0476\n",
      "Epoch 203/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 0.8583 - accuracy: 0.7878 - val_loss: 17.1595 - val_accuracy: 0.0457\n",
      "Epoch 204/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8578 - accuracy: 0.7900 - val_loss: 17.1764 - val_accuracy: 0.0462\n",
      "Epoch 205/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8502 - accuracy: 0.7923 - val_loss: 17.2084 - val_accuracy: 0.0466\n",
      "Epoch 206/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8515 - accuracy: 0.7890 - val_loss: 17.1828 - val_accuracy: 0.0462\n",
      "Epoch 207/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8478 - accuracy: 0.7877 - val_loss: 17.2105 - val_accuracy: 0.0462\n",
      "Epoch 208/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8497 - accuracy: 0.7892 - val_loss: 17.2268 - val_accuracy: 0.0451\n",
      "Epoch 209/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8512 - accuracy: 0.7889 - val_loss: 17.2451 - val_accuracy: 0.0466\n",
      "Epoch 210/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8392 - accuracy: 0.7922 - val_loss: 17.2695 - val_accuracy: 0.0484\n",
      "Epoch 211/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8354 - accuracy: 0.7941 - val_loss: 17.3053 - val_accuracy: 0.0468\n",
      "Epoch 212/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8321 - accuracy: 0.7926 - val_loss: 17.3659 - val_accuracy: 0.0462\n",
      "Epoch 213/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8308 - accuracy: 0.7942 - val_loss: 17.3389 - val_accuracy: 0.0437\n",
      "Epoch 214/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8250 - accuracy: 0.7947 - val_loss: 17.3258 - val_accuracy: 0.0449\n",
      "Epoch 215/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.8314 - accuracy: 0.7934 - val_loss: 17.3532 - val_accuracy: 0.0482\n",
      "Epoch 216/250\n",
      "644/644 [==============================] - 7s 12ms/step - loss: 0.8258 - accuracy: 0.7933 - val_loss: 17.3875 - val_accuracy: 0.0470\n",
      "Epoch 217/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8205 - accuracy: 0.7936 - val_loss: 17.4537 - val_accuracy: 0.0462\n",
      "Epoch 218/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8212 - accuracy: 0.7952 - val_loss: 17.3960 - val_accuracy: 0.0472\n",
      "Epoch 219/250\n",
      "644/644 [==============================] - 7s 11ms/step - loss: 0.8191 - accuracy: 0.7938 - val_loss: 17.4901 - val_accuracy: 0.0490\n",
      "Epoch 220/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.8142 - accuracy: 0.7985 - val_loss: 17.4788 - val_accuracy: 0.0466\n",
      "Epoch 221/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.8253 - accuracy: 0.7935 - val_loss: 17.5270 - val_accuracy: 0.0478\n",
      "Epoch 222/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.8157 - accuracy: 0.7946 - val_loss: 17.5338 - val_accuracy: 0.0443\n",
      "Epoch 223/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.8144 - accuracy: 0.7973 - val_loss: 17.5095 - val_accuracy: 0.0457\n",
      "Epoch 224/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.8060 - accuracy: 0.7980 - val_loss: 17.5592 - val_accuracy: 0.0449\n",
      "Epoch 225/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.8103 - accuracy: 0.7949 - val_loss: 17.5744 - val_accuracy: 0.0472\n",
      "Epoch 226/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.8085 - accuracy: 0.7987 - val_loss: 17.5688 - val_accuracy: 0.0460\n",
      "Epoch 227/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.8029 - accuracy: 0.7990 - val_loss: 17.6079 - val_accuracy: 0.0488\n",
      "Epoch 228/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.8057 - accuracy: 0.7977 - val_loss: 17.6245 - val_accuracy: 0.0462\n",
      "Epoch 229/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7968 - accuracy: 0.7979 - val_loss: 17.6450 - val_accuracy: 0.0439\n",
      "Epoch 230/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7967 - accuracy: 0.7998 - val_loss: 17.6913 - val_accuracy: 0.0470\n",
      "Epoch 231/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7958 - accuracy: 0.7988 - val_loss: 17.6363 - val_accuracy: 0.0449\n",
      "Epoch 232/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7912 - accuracy: 0.8022 - val_loss: 17.6386 - val_accuracy: 0.0460\n",
      "Epoch 233/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7874 - accuracy: 0.8019 - val_loss: 17.6750 - val_accuracy: 0.0484\n",
      "Epoch 234/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7824 - accuracy: 0.8026 - val_loss: 17.7434 - val_accuracy: 0.0486\n",
      "Epoch 235/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7886 - accuracy: 0.8007 - val_loss: 17.7372 - val_accuracy: 0.0480\n",
      "Epoch 236/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7793 - accuracy: 0.8029 - val_loss: 17.8098 - val_accuracy: 0.0480\n",
      "Epoch 237/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7752 - accuracy: 0.8058 - val_loss: 17.7839 - val_accuracy: 0.0480\n",
      "Epoch 238/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7872 - accuracy: 0.8032 - val_loss: 17.7901 - val_accuracy: 0.0439\n",
      "Epoch 239/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7860 - accuracy: 0.8008 - val_loss: 17.8553 - val_accuracy: 0.0472\n",
      "Epoch 240/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7727 - accuracy: 0.8060 - val_loss: 17.8986 - val_accuracy: 0.0495\n",
      "Epoch 241/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7771 - accuracy: 0.8044 - val_loss: 17.8564 - val_accuracy: 0.0499\n",
      "Epoch 242/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7781 - accuracy: 0.8035 - val_loss: 17.8884 - val_accuracy: 0.0464\n",
      "Epoch 243/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7719 - accuracy: 0.8024 - val_loss: 17.8873 - val_accuracy: 0.0459\n",
      "Epoch 244/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7693 - accuracy: 0.8047 - val_loss: 17.9676 - val_accuracy: 0.0459\n",
      "Epoch 245/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7663 - accuracy: 0.8061 - val_loss: 17.9193 - val_accuracy: 0.0478\n",
      "Epoch 246/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7740 - accuracy: 0.8037 - val_loss: 17.9885 - val_accuracy: 0.0493\n",
      "Epoch 247/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7634 - accuracy: 0.8055 - val_loss: 18.0177 - val_accuracy: 0.0482\n",
      "Epoch 248/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7649 - accuracy: 0.8051 - val_loss: 18.0039 - val_accuracy: 0.0437\n",
      "Epoch 249/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7576 - accuracy: 0.8063 - val_loss: 18.0349 - val_accuracy: 0.0439\n",
      "Epoch 250/250\n",
      "644/644 [==============================] - 8s 12ms/step - loss: 0.7653 - accuracy: 0.8061 - val_loss: 18.0165 - val_accuracy: 0.0455\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(x_train, \n",
    "                    y_train,\n",
    "                    epochs = 250,\n",
    "                    validation_data = (x_test, y_test),\n",
    "                    verbose = 1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the next word\n",
    "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    if len(token_list) >= max_sequence_len:\n",
    "        token_list = token_list[-(max_sequence_len-1):] # Ensure the sequence length matches the max sequence legth -1\n",
    "    token_list = pad_sequences([token_list], maxlen = max_sequence_len-1, padding= 'pre')\n",
    "    predicted = model.predict(token_list, verbose = 0)\n",
    "    predicted_word_index = np.argmax(predicted, axis = 1)\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Honesty is the best \n",
      "Next Word Prediction: of\n"
     ]
    }
   ],
   "source": [
    "input_text = 'Honesty is the best '\n",
    "print(f'Input Text: {input_text}')\n",
    "max_sequence_len = model.input_shape[1]+1\n",
    "next_word = predict_next_word(model, tokenizer, input_text, max_sequence_len)\n",
    "print(f'Next Word Prediction: {next_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/Desktop/GenerativeAI/ANN/venv/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('next_word_lstm.h5')\n",
    "\n",
    "# Save the tokenizer\n",
    "import pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:  Bar. 'Tis now strook twelue, get thee to bed\n",
      "Next Word Prediction: francisco\n"
     ]
    }
   ],
   "source": [
    "input_text = \" Bar. 'Tis now strook twelue, get thee to bed\"\n",
    "print(f'Input Text: {input_text}')\n",
    "max_sequence_len = model.input_shape[1]+1\n",
    "next_word = predict_next_word(model, tokenizer, input_text, max_sequence_len)\n",
    "print(f'Next Word Prediction: {next_word}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
